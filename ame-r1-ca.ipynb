{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJtj6mqk_hid"
   },
   "outputs": [],
   "source": [
    "%pip install torch transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0FOpuwp_hie"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, expansion_factor: float = 16):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = int(input_dim * expansion_factor)\n",
    "        self.decoder = nn.Linear(self.latent_dim, input_dim, bias=True)\n",
    "        self.encoder = nn.Linear(input_dim, self.latent_dim, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        encoded = F.relu(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return F.relu(self.encoder(x))\n",
    "\n",
    "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return self.decoder(x)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path: str, input_dim: int, expansion_factor: float = 16, device: str = \"cuda\") -> \"SparseAutoencoder\":\n",
    "        model = cls(input_dim=input_dim, expansion_factor=expansion_factor)\n",
    "        state_dict = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-wCVRO__hie",
    "outputId": "d6ae6e44-6f46-44bc-8e0a-bf75f0383d47"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download, notebook_login\n",
    "sae_name = \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=f\"qresearch/{sae_name}\",\n",
    "    filename=f\"{sae_name}.pt\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMbNZI3J_hif"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"bfloat16\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sae = SparseAutoencoder.from_pretrained(\n",
    "    path=file_path,\n",
    "    input_dim=model.config.hidden_size,\n",
    "    expansion_factor=16,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ze2O8P4e_hig",
    "outputId": "047731ed-ac2d-4c3e-f644-7f22faec5ff5"
   },
   "outputs": [],
   "source": [
    "def generate_with_intervention(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sae,\n",
    "    messages: list[dict],\n",
    "    feature_idx: int,\n",
    "    neg_feature_idx: int,\n",
    "    intervention: float = 3.0,\n",
    "    neg_intervention: float = 3.0,\n",
    "    target_layer: int = 9,\n",
    "    max_new_tokens: int = 50\n",
    "):\n",
    "    modified_activations = None\n",
    "\n",
    "    def intervention_hook(module, inputs, outputs):\n",
    "        nonlocal modified_activations\n",
    "        activations = inputs[0]\n",
    "\n",
    "        features = sae.encode(activations.to(torch.float32))\n",
    "        reconstructed = sae.decode(features)\n",
    "        error = activations.to(torch.float32) - reconstructed\n",
    "\n",
    "        features[:, :, feature_idx] += intervention\n",
    "        features[:, :, neg_feature_idx] -= neg_intervention\n",
    "\n",
    "        modified = sae.decode(features) + error\n",
    "        modified_activations = modified\n",
    "        modified_activations = modified.to(torch.bfloat16)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def output_hook(module, inputs, outputs):\n",
    "        nonlocal modified_activations\n",
    "        if modified_activations is not None:\n",
    "            return (modified_activations,) + outputs[1:] if len(outputs) > 1 else (modified_activations,)\n",
    "        return outputs\n",
    "\n",
    "    handles = [\n",
    "        model.model.layers[target_layer].register_forward_hook(intervention_hook),\n",
    "        model.model.layers[target_layer].register_forward_hook(output_hook)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        input_tokens = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_tokens,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False  # Use greedy decoding for consistency\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    finally:\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How's it going?\"}\n",
    "]\n",
    "\n",
    "AMERICAN_FEATURE = 8187\n",
    "CCP_FEATURE = 44968\n",
    "\n",
    "print(\"Steering with AME(R1)CA:\")\n",
    "output_text = generate_with_intervention(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    sae=sae,\n",
    "    messages=messages,\n",
    "    feature_idx=AMERICAN_FEATURE,\n",
    "    neg_feature_idx=CCP_FEATURE,\n",
    "    intervention=8,\n",
    "    neg_intervention=1,\n",
    "    target_layer=19,\n",
    "    max_new_tokens=70\n",
    ")\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
